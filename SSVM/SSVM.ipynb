{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "datapath = 'data/';\n",
    "files = os.listdir(datapath)\n",
    "def readFile(s):\n",
    "    fh = open(datapath+files[s],'r')\n",
    "    rawlines = fh.readlines()\n",
    "    lines = [line.strip('\\n').split(',') for line in rawlines];\n",
    "    fh.close();\n",
    "    #ys is the tags for the word to distinct {verb, noun, adjective, adverb, preposition, pronoun, determiner, number, punctuation, other}\n",
    "    ys = [int(l[1])-1 for l in lines];\n",
    "    #xs is some features of the word\n",
    "    xs = [[int(l[2])-1,int(l[3]),int(l[4]),int(l[5])-1,int(l[6])-1] for l in lines];\n",
    "    return ys, xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00091917  0.00050669  0.00072867  0.00030579  0.00090685  0.00027299\n",
      "  0.00043459  0.00039775  0.00084228  0.00069654]\n"
     ]
    }
   ],
   "source": [
    "import pyGM as gm\n",
    "from pyGM import wmb\n",
    "# the number of possible value for each feature in x\n",
    "feature_sizes = [1,2,2,201,201]\n",
    "#the random factor table for y vs x\n",
    "ThetaF = [.001*np.random.rand(10,feature_sizes[f]) for f in range(len(feature_sizes))];\n",
    "ThetaP = .001*np.random.rand(10,10);\n",
    "Loss = 1.0 - np.eye(10);\n",
    "\n",
    "num_iter = 5\n",
    "hamming = np.zeros(num_iter)\n",
    "hinge = np.zeros(num_iter)\n",
    "eta = 0.01\n",
    "alpha = 0.5\n",
    "print (ThetaF[1][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1\n",
      "Hamming Loss: 0.214035537331\n",
      "Hinge Loss: 0.35472984079\n",
      "Hamming Loss: 0.189165016291\n",
      "Hinge Loss: 0.328147767777\n",
      "Hamming Loss: 0.17969533114\n",
      "Hinge Loss: 0.318159290174\n",
      "Hamming Loss: 0.175443244561\n",
      "Hinge Loss: 0.313868392742\n",
      "Hamming Loss: 0.172144785318\n",
      "Hinge Loss: 0.309634110046\n",
      "Hamming Loss: 0.170705385353\n",
      "Hinge Loss: 0.30837991972\n",
      "Hamming Loss: 0.17071958536\n",
      "Hinge Loss: 0.308403990904\n",
      "iteration: 2\n",
      "Hamming Loss: 0.157870114405\n",
      "Hinge Loss: 0.293292049359\n",
      "Hamming Loss: 0.160832605407\n",
      "Hinge Loss: 0.297792016089\n",
      "Hamming Loss: 0.159881158843\n",
      "Hinge Loss: 0.296520737171\n",
      "Hamming Loss: 0.16015944725\n",
      "Hinge Loss: 0.297149909949\n",
      "Hamming Loss: 0.159874581514\n",
      "Hinge Loss: 0.296650060622\n",
      "Hamming Loss: 0.160133480067\n",
      "Hinge Loss: 0.297272907065\n",
      "Hamming Loss: 0.160168980084\n",
      "Hinge Loss: 0.29733358672\n",
      "iteration: 3\n",
      "Hamming Loss: 0.164468911364\n",
      "Hinge Loss: 0.304327407501\n",
      "Hamming Loss: 0.16047970167\n",
      "Hinge Loss: 0.298034652409\n",
      "Hamming Loss: 0.161490858741\n",
      "Hinge Loss: 0.300404680944\n",
      "Hamming Loss: 0.160821105645\n",
      "Hinge Loss: 0.298803021997\n",
      "Hamming Loss: 0.16098233432\n",
      "Hinge Loss: 0.298677573433\n",
      "Hamming Loss: 0.161255280628\n",
      "Hinge Loss: 0.299220957694\n",
      "Hamming Loss: 0.161297880649\n",
      "Hinge Loss: 0.299295518789\n",
      "iteration: 4\n",
      "Hamming Loss: 0.161755990179\n",
      "Hinge Loss: 0.299209341066\n",
      "Hamming Loss: 0.16246106909\n",
      "Hinge Loss: 0.299874770596\n",
      "Hamming Loss: 0.161378880933\n",
      "Hinge Loss: 0.298025579554\n",
      "Hamming Loss: 0.161700542726\n",
      "Hinge Loss: 0.298852417361\n",
      "Hamming Loss: 0.160722914035\n",
      "Hinge Loss: 0.297525506441\n",
      "Hamming Loss: 0.161049380525\n",
      "Hinge Loss: 0.298378944175\n",
      "Hamming Loss: 0.161084880542\n",
      "Hinge Loss: 0.298435057869\n",
      "iteration: 5\n",
      "Hamming Loss: 0.165938681226\n",
      "Hinge Loss: 0.305926719068\n",
      "Hamming Loss: 0.160832308238\n",
      "Hinge Loss: 0.298016536624\n",
      "Hamming Loss: 0.162236933109\n",
      "Hinge Loss: 0.299637065904\n",
      "Hamming Loss: 0.160640353298\n",
      "Hinge Loss: 0.297464724659\n",
      "Hamming Loss: 0.161032768247\n",
      "Hinge Loss: 0.297783998618\n",
      "Hamming Loss: 0.16090028045\n",
      "Hinge Loss: 0.29830727246\n",
      "Hamming Loss: 0.160921580461\n",
      "Hinge Loss: 0.29834131521\n"
     ]
    }
   ],
   "source": [
    "for iter in range(num_iter):\n",
    "    print ('iteration:',iter+1)\n",
    "    N = 0\n",
    "    num_files = 0\n",
    "    for s in np.random.permutation(len(files)-1):\n",
    "        #load data ys, xs\n",
    "        ys,xs = readFile(s+1)\n",
    "        ns = len(ys)\n",
    "        #updata the number of words\n",
    "        N +=ns\n",
    "        num_files +=1\n",
    "        #Define random variables for the inference process:\n",
    "        Y = [gm.Var(i,10) for i in range(ns)]\n",
    "        # Build \"prediction model\" using parameters\n",
    "        factors = []\n",
    "        for i in range(ns):\n",
    "            for j in range(len(feature_sizes)):\n",
    "            #record factor between y and x\n",
    "                factor = gm.Factor([Y[i]],1)\n",
    "                factor.table = np.exp(ThetaF[j][:,xs[i][j]])\n",
    "                factors.append(factor)\n",
    "            if i < ns-1:\n",
    "            #record factors between yi and yi+1\n",
    "                factor = gm.Factor([Y[i],Y[i+1]],1)\n",
    "                factor.table = np.exp(ThetaP)\n",
    "                factors.append(factor)\n",
    "        model_pred = gm.GraphModel(factors)\n",
    "        # Copy factors and add extra Hamming factors for loss-augmented model\n",
    "        factors_aug = [ f for f in factors ]\n",
    "        factors_aug.extend( [gm.Factor([Y[i]],Loss[:,ys[i]]).exp() for i in range(ns)] );\n",
    "        model_aug = gm.GraphModel(factors_aug);\n",
    "        order = range(ns);  # eliminate in sequence (Markov chain on y)\n",
    "        wt = 1e-4;         # for max elimination in JTree implementation\n",
    "        # Now, the most likely configuration of the prediction model (for prediction) is:\n",
    "        yhat_pred = wmb.JTree(model_pred,order,wt).argmax();\n",
    "        yhat_pred = [yhat_pred[i] for i in yhat_pred]\n",
    "        # and the maximizing argument of the loss (for computing the gradient) is\n",
    "        yhat_aug = wmb.JTree(model_aug,order,wt).argmax();\n",
    "        yhat_aug = [yhat_aug[i] for i in yhat_aug]\n",
    "        # use yhat_pred & ys to keep a running estimate of your prediction accuracy & print it\n",
    "        if num_files%1000==0:\n",
    "            print (\"Hamming Loss:\", hamming[iter]/N)\n",
    "            print (\"Hinge Loss:\", hinge[iter]/N)\n",
    "                \n",
    "        #hamming loss\n",
    "        h = Loss[np.asarray(ys).astype(int),np.asarray(yhat_pred).astype(int)].sum()\n",
    "        #h = Loss[ys,yhat_pred].sum()\n",
    "        hamming[iter] +=h\n",
    "        #hinge loss\n",
    "        score_y_pred, score_ys = 0,0\n",
    "        for i in range(ns):\n",
    "            for j in range(len(feature_sizes)):\n",
    "                score_y_pred +=ThetaF[j][yhat_pred[i]][xs[i][j]]\n",
    "                score_ys +=ThetaF[j][ys[i]][xs[i][j]]\n",
    "            if i < ns-1:\n",
    "                score_y_pred +=ThetaP[yhat_pred[i],yhat_pred[i+1]]\n",
    "                score_ys +=ThetaP[ys[i],ys[i+1]]\n",
    "                    \n",
    "        norms = [np.linalg.norm(ThetaF[j]) for j in range(len(feature_sizes))]\n",
    "        norms.append(np.linalg.norm(ThetaP))\n",
    "        #norms = np.asarray(norms)\n",
    "        hinge[iter] +=h +score_y_pred - score_ys + eta*np.dot(norms,norms)\n",
    "            \n",
    "        GradF = [np.zeros((10,feature_sizes[f])) for f in range(len(feature_sizes))]\n",
    "        GradP = np.zeros((10,10))\n",
    "            \n",
    "        for i in range(ns):\n",
    "            for j in range(len(feature_sizes)):\n",
    "                GradF[j][yhat_aug[i]][xs[i][j]] +=alpha\n",
    "                GradF[j][ys[i]][xs[i][j]] -=alpha\n",
    "            if i < ns-1:\n",
    "                GradP[yhat_aug[i],yhat_aug[i+1]] +=alpha\n",
    "                GradP[ys[i],ys[i+1]] -=alpha\n",
    "                    \n",
    "        GradF = [2*eta*ThetaF[j] + GradF[j] for j in range(len(feature_sizes))]\n",
    "        GradP = 2 * eta * ThetaP + GradP \n",
    "        \n",
    "        ThetaF = [ThetaF[j] - GradF[j]/ns for j in range(len(feature_sizes))]\n",
    "        ThetaP = ThetaP - GradP/ns\n",
    "            \n",
    "    hamming[iter] /=N\n",
    "    hinge[iter] /=N\n",
    "    print (\"Hamming Loss:\",hamming[iter])\n",
    "    print (\"Hinge Loss:\",hinge[iter])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
